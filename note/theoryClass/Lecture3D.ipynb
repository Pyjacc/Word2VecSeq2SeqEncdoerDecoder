{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 模型Layer、Model构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.1 一些tensor操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.concat ：矩阵拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=22, shape=(2, 6), dtype=int32, numpy=\n",
       "array([[ 1,  2,  3,  7,  8,  9],\n",
       "       [ 4,  5,  6, 10, 11, 12]], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = [[1, 2, 3], [4, 5, 6]] # 2, 3\n",
    "t2 = [[7, 8, 9], [10, 11, 12]] \n",
    "tf.concat([t1, t2], axis=1) #axis=1时，将t2从水平方向拼接到t1的右边。axis=0，将t2从垂直方向拼接到t1下面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.expand_dims ：增加维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t3 = [[1, 2, 3],[4, 5, 6]] # shape [2, 3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=39, shape=(1, 2, 3), dtype=int32, numpy=\n",
       "array([[[1, 2, 3],\n",
       "        [4, 5, 6]]], dtype=int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(t3, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=42, shape=(2, 1, 3), dtype=int32, numpy=\n",
       "array([[[1, 2, 3]],\n",
       "\n",
       "       [[4, 5, 6]]], dtype=int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(t3, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=45, shape=(2, 3, 1), dtype=int32, numpy=\n",
       "array([[[1],\n",
       "        [2],\n",
       "        [3]],\n",
       "\n",
       "       [[4],\n",
       "        [5],\n",
       "        [6]]], dtype=int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(t3, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.squeeze ：删除维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t4 = tf.expand_dims(t3, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=48, shape=(2, 3, 1), dtype=int32, numpy=\n",
       "array([[[1],\n",
       "        [2],\n",
       "        [3]],\n",
       "\n",
       "       [[4],\n",
       "        [5],\n",
       "        [6]]], dtype=int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=49, shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze(t4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.cast ：转换类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=51, shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([1.8, 2.2], dtype=tf.float32)\n",
    "tf.dtypes.cast(x, tf.int32) \n",
    "# mask = [True , False] loss.astype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.stack ：堆积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant([1, 4]) \n",
    "y = tf.constant([2, 5]) \n",
    "z = tf.constant([3, 6]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=55, shape=(3, 2), dtype=int32, numpy=\n",
       "array([[1, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([x, y, z], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=56, shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([x, y, z], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    # enc_units：RNN，LSTM等单元数量\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, embedding_matrix):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        #将一个特征转换为一个向量。比如最容易理解的one-hot编码。但在实际应用当中，将特征转换为one-hot编码后维度会十分高。\n",
    "        #所以我们会将one-hot这种稀疏特征转化为稠密特征，通常做法也就是转化为我们常用的embedding\n",
    "        # weights：词向量矩阵，trainable：是否进行训练\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                                                   embedding_dim,\n",
    "                                                   weights=[embedding_matrix],\n",
    "                                                   trainable=False)\n",
    "        # 创建gru单元\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    #进行encoder计算\n",
    "    #x：输入词的词向量，hidden：前一个词的状态变量\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)  #词向量的embedding，即转换为稠密特征\n",
    "        output, state = self.gru(x, initial_state=hidden) #output, state对应gru的两个输出\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    # dec_hidden：decoder hidden state，每一步（一个词）都不一样，因此每步传入的都不一样\n",
    "    # enc_output：encoder output：每一个batch_size训练中，训练完最后一个词后统一输出endoer output，因此在每一步训练中\n",
    "    # enc_output是不变的。enc_output中包含了sequence_len的值\n",
    "    def call(self, dec_hidden, enc_output):\n",
    "        \"\"\"\n",
    "        :param dec_hidden: shape=(16, 256)\n",
    "        :param enc_output: shape=(16, 4, 256) #4：sequence_len=4，即每个样本中有4个词\n",
    "        \"\"\"\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size) #每个词对应的的hidden state的维度\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(dec_hidden, 1)  # shape=(16, 1, 256)，维度抓换，为了后面的矩阵运算score\n",
    "        # att_features = self.W1(enc_output) + self.W2(hidden_with_time_axis)\n",
    "\n",
    "        # Calculate v^T tanh(W_h h_i + W_s s_t + b_attn)  #b_attn：偏置量，可加可不加\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))  # shape=(16, 200, 1)\n",
    "        attn_dist = tf.nn.softmax(score, axis=1) #attention weights\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size, 1)\n",
    "        attn_dist = tf.expand_dims(attn_dist, axis=2)\n",
    "        # 每个词的attention weights和enc_output相乘，再相加即得到context vector\n",
    "        # 矩阵运算，已经包含了所有词的attn_dist * enc_output\n",
    "        context_vector = attn_dist * enc_output  # shape=(16, 200, 256)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)  # shape=(16, 256)\n",
    "        return context_vector, tf.squeeze(attn_dist, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, embedding_matrix):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                                   weights=[embedding_matrix],\n",
    "                                                   trainable=False)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        # fc层.\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size, activation=tf.keras.activations.softmax)\n",
    "\n",
    "    # 开始计算\n",
    "    # x：decoder输入词的词向量\n",
    "    def call(self, x, context_vector):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) # 与context vecotr进行拼接\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # output shape == (batch_size, vocab)\n",
    "        out = self.fc(output)\n",
    "        return x, out, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Model  \n",
    "搭建seq2seq模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-31-07f725cf71c5>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-07f725cf71c5>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    def call(self, enc_output, dec_hidden, enc_inp, dec_inp):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#Model具有反向传播等特性\n",
    "class SEQ2SEQ(tf.keras.Model):\n",
    "    # 没写完整\n",
    "    def __init__(self):\n",
    "        \n",
    "    \n",
    "    def call(self, enc_output, dec_hidden, enc_inp, dec_inp):\n",
    "        predictions = []\n",
    "        attentions = []\n",
    "\n",
    "        context_vector, attn_dist = self.attention(dec_hidden,  # shape=(16, 256)\n",
    "                                                    enc_output,  # shape=(16, 200, 256)\n",
    "                                                    )\n",
    "\n",
    "        for t in range(dec_inp.shape[1]):\n",
    "            #传入的参数与Attention机制图的参数对应\n",
    "            #dec_inp:decoder 输入, dec_hidden:前一个词的状态变量，enc_output：encoder输出\n",
    "#             dec_x, pred, dec_hidden = self.decoder(tf.expand_dims(dec_inp[:, t], 1),\n",
    "#                                                    dec_hidden,\n",
    "#                                                    enc_output,\n",
    "#                                                    context_vector)\n",
    "\n",
    "            #由于前面定义decoder层时（call(self, x, context_vector):），只用到了decoder的输入词和context_vector，\n",
    "            #因此此处只用两个参数\n",
    "            dec_x, pred, dec_hidden = self.decoder(tf.expand_dims(dec_inp[:, t], 1),\n",
    "                                                   context_vector)\n",
    "            #每训练一步，dec_hidden都要重新输入\n",
    "            context_vector, attn_dist, coverage_next = self.attention(dec_hidden, enc_output)\n",
    "            predictions.append(pred)    #预测值\n",
    "            attentions.append(attn_dist)\n",
    " \n",
    "        return tf.stack(predictions, 1), dec_hidden, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.09458993\n"
     ]
    }
   ],
   "source": [
    "#分类交叉熵\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "loss = cce(\n",
    "  [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]], # real，真实值（标签）\n",
    "  [[.9, .05, .05], [.05, .89, .06], [.05, .01, .94]]) # prediction，预测值\n",
    "print('Loss: ', loss.numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'op'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-c939952ee125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m loss = sce(\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# shape=(3,) real index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpStage01/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    124\u001b[0m         y_true, y_pred, sample_weight)\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[1;32m    128\u001b[0m           losses, sample_weight, reduction=self._get_reduction())\n",
      "\u001b[0;32m~/anaconda3/envs/nlpStage01/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    219\u001b[0m       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n\u001b[1;32m    220\u001b[0m           y_pred, y_true)\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpStage01/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, axis)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m   return K.sparse_categorical_crossentropy(\n\u001b[0;32m--> 978\u001b[0;31m       y_true, y_pred, from_logits=from_logits, axis=axis)\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpStage01/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   4501\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4502\u001b[0m     if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or\n\u001b[0;32m-> 4503\u001b[0;31m         output.op.type != 'Softmax'):\n\u001b[0m\u001b[1;32m   4504\u001b[0m       \u001b[0mepsilon_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constant_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4505\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'op'"
     ]
    }
   ],
   "source": [
    "#稀疏分类交叉熵\n",
    "# 上课老师代码就报错\n",
    "sce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "loss = sce(\n",
    "  [1, 2], # shape=(3,) real index，真实值可以不传其one-hot编码，传其在字典中的index\n",
    "  [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
    "print('Loss: ', loss.numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e8ed13e00473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 优化器调用api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m optimizer = tf.keras.optimizers.Adagrad(params['learning_rate'],\n\u001b[0m\u001b[1;32m      3\u001b[0m                                         \u001b[0minitial_accumulator_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adagrad_init_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                         clipnorm=params['max_grad_norm'])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "# 优化器调用api\n",
    "optimizer = tf.keras.optimizers.Adagrad(params['learning_rate'],\n",
    "                                        initial_accumulator_value=params['adagrad_init_acc'],\n",
    "                                        clipnorm=params['max_grad_norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss调用api\n",
    "# from_logits与decoder层中fc层的activation对应，activation设置了激活函数，则此处也就为True，如果没有激活函数，则此处为False\n",
    "# reduction参数不用管\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "# Whether y_pred is expected to be a logits tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用了交叉熵api的LOSS函数  \n",
    "自定义loss函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义loss函数\n",
    "[2, 4, 5, 6, 7, 1, 1, 1, 1, 1]\n",
    "def loss_function(real, pred):\n",
    "    # 判断logit为1和0的数量\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 1))\n",
    "    # 计算decoder的长度\n",
    "    dec_lens = tf.reduce_sum(tf.cast(mask, dtype=tf.float32), axis=-1)\n",
    "    # 计算loss值\n",
    "    loss_ = loss_object(real, pred)\n",
    "    # 转换mask的格式\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    # 调整loss\n",
    "    loss_ *= mask\n",
    "    # 确认下是否有空的摘要别加入计算\n",
    "    loss_ = tf.reduce_sum(loss_, axis=-1) / dec_lens\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自己diy的LOSS函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pgn_log_loss_function(real, final_dists, padding_mask):\n",
    "    # This is fiddly; we use tf.gather_nd to pick out the probabilities of the gold target words\n",
    "    loss_per_step = []  # will be list length max_dec_steps containing shape (batch_size)\n",
    "    batch_nums = tf.range(0, limit=real.shape[0])  # shape (batch_size)\n",
    "    for dec_step, dist in enumerate(final_dists):\n",
    "        # The indices of the target words. shape (batch_size)\n",
    "        targets = real[:, dec_step]\n",
    "        indices = tf.stack((batch_nums, targets), axis=1)  # shape (batch_size, 2)\n",
    "        gold_probs = tf.gather_nd(dist, indices)  # shape (batch_size). prob of correct words on this step\n",
    "        losses = -tf.math.log(gold_probs)\n",
    "        loss_per_step.append(losses)\n",
    "    # Apply dec_padding_mask and get loss\n",
    "    _loss = _mask_and_avg(loss_per_step, padding_mask)\n",
    "    return _loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立train-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    # 初始化loss\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        # encoder实例化，同时也是初始化的过程\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        # 确定decoder的输入\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        # 开始train的过程\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            # 计算第一步输出后的loss值\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    # 计算一个训练样本的loss\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    # 反向梯度求导\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(batch_loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpStage01",
   "language": "python",
   "name": "nlpstage01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
